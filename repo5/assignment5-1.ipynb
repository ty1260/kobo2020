{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第5回レポート課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず, SimpleConvNetクラスを実装した."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y  =self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルコードを用いてMNISTデータの学習を行った."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299832360620802\n",
      "=== epoch:1, train acc:0.222, test acc:0.248 ===\n",
      "train loss:2.2974436989266986\n",
      "train loss:2.29310997193581\n",
      "train loss:2.286003002095219\n",
      "train loss:2.279860466133428\n",
      "train loss:2.267502576946028\n",
      "train loss:2.258001005198759\n",
      "train loss:2.233651995484623\n",
      "train loss:2.2191559518750883\n",
      "train loss:2.1949721778899924\n",
      "train loss:2.137368809169624\n",
      "train loss:2.123454716856383\n",
      "train loss:2.0730121466028524\n",
      "train loss:1.9958706740645398\n",
      "train loss:1.9705548308912464\n",
      "train loss:1.879298774003408\n",
      "train loss:1.8425366220603927\n",
      "train loss:1.7722960258508422\n",
      "train loss:1.724992256693547\n",
      "train loss:1.546803898096503\n",
      "train loss:1.5921264278131348\n",
      "train loss:1.4507208147124089\n",
      "train loss:1.3739387359606363\n",
      "train loss:1.3613255933516226\n",
      "train loss:1.2843633583705951\n",
      "train loss:1.195566858598652\n",
      "train loss:1.1222828915105532\n",
      "train loss:1.0467450708641157\n",
      "train loss:1.0193290623593265\n",
      "train loss:1.183286320362447\n",
      "train loss:0.8238484166434523\n",
      "train loss:1.0551939098199297\n",
      "train loss:0.6922153038592849\n",
      "train loss:0.7602639651703089\n",
      "train loss:0.7147212013216582\n",
      "train loss:0.6242957592345197\n",
      "train loss:0.7100344589060118\n",
      "train loss:0.6940386210751851\n",
      "train loss:0.5816676831724873\n",
      "train loss:0.6597940693979483\n",
      "train loss:0.6543838054504648\n",
      "train loss:0.546457119418246\n",
      "train loss:0.5541837975631129\n",
      "train loss:0.7286214332230067\n",
      "train loss:0.4176764439942918\n",
      "train loss:0.5155178147938178\n",
      "train loss:0.5553539795135967\n",
      "train loss:0.5005460266069599\n",
      "train loss:0.5762180233176422\n",
      "train loss:0.5373657496598774\n",
      "train loss:0.5025109905372649\n",
      "=== epoch:2, train acc:0.812, test acc:0.825 ===\n",
      "train loss:0.47031476156627783\n",
      "train loss:0.47774540907312485\n",
      "train loss:0.39519730165055333\n",
      "train loss:0.5166248237742608\n",
      "train loss:0.44873631796719593\n",
      "train loss:0.391853618391589\n",
      "train loss:0.35173974177311496\n",
      "train loss:0.4657407688116211\n",
      "train loss:0.3793074136655036\n",
      "train loss:0.4782776409235641\n",
      "train loss:0.46335073854382847\n",
      "train loss:0.48229684240682097\n",
      "train loss:0.3154014739731473\n",
      "train loss:0.41932338702438754\n",
      "train loss:0.40197561804290255\n",
      "train loss:0.450568688761435\n",
      "train loss:0.44461828914952983\n",
      "train loss:0.5608082521946754\n",
      "train loss:0.5033686422812653\n",
      "train loss:0.2904121974511729\n",
      "train loss:0.3861556655930745\n",
      "train loss:0.39253219999504907\n",
      "train loss:0.40872800760514144\n",
      "train loss:0.30902890421223256\n",
      "train loss:0.6660742738843074\n",
      "train loss:0.6210500848130526\n",
      "train loss:0.2854884668436944\n",
      "train loss:0.3577641090563219\n",
      "train loss:0.3082281730100306\n",
      "train loss:0.3170545968101611\n",
      "train loss:0.39157108178518496\n",
      "train loss:0.4008274327562313\n",
      "train loss:0.3498613378230577\n",
      "train loss:0.25626584455974855\n",
      "train loss:0.5158109229904043\n",
      "train loss:0.46316898085298946\n",
      "train loss:0.4476533493387798\n",
      "train loss:0.32583605810283617\n",
      "train loss:0.367131377228371\n",
      "train loss:0.40771488949372825\n",
      "train loss:0.17802320936221463\n",
      "train loss:0.2880183779062954\n",
      "train loss:0.43496268482012185\n",
      "train loss:0.2376682967378697\n",
      "train loss:0.4449211861281973\n",
      "train loss:0.28973166250084215\n",
      "train loss:0.3213777783334173\n",
      "train loss:0.36938735788964167\n",
      "train loss:0.321277293720628\n",
      "train loss:0.3275734220008919\n",
      "=== epoch:3, train acc:0.853, test acc:0.858 ===\n",
      "train loss:0.3547869363785256\n",
      "train loss:0.4282715319279556\n",
      "train loss:0.37839236050875646\n",
      "train loss:0.3418465295812604\n",
      "train loss:0.26514143356487163\n",
      "train loss:0.3606275915030184\n",
      "train loss:0.18456656032793972\n",
      "train loss:0.24346926999091875\n",
      "train loss:0.29534427537062363\n",
      "train loss:0.36189950576504437\n",
      "train loss:0.27630749865385296\n",
      "train loss:0.31657588879336357\n",
      "train loss:0.33906564139768575\n",
      "train loss:0.3293322071975001\n",
      "train loss:0.32826764237086403\n",
      "train loss:0.25962722467224103\n",
      "train loss:0.3449385174652131\n",
      "train loss:0.303514233096972\n",
      "train loss:0.2941205513873229\n",
      "train loss:0.2920073631654945\n",
      "train loss:0.2721403020997715\n",
      "train loss:0.4191749641660141\n",
      "train loss:0.44628001571079196\n",
      "train loss:0.20652140274934577\n",
      "train loss:0.3303550105723057\n",
      "train loss:0.23932682109131398\n",
      "train loss:0.22103575978822168\n",
      "train loss:0.34072033312904826\n",
      "train loss:0.3198407425861182\n",
      "train loss:0.2349883887657812\n",
      "train loss:0.26744610138059083\n",
      "train loss:0.30651284385563254\n",
      "train loss:0.3502156820744527\n",
      "train loss:0.1320651709917575\n",
      "train loss:0.6079276371226796\n",
      "train loss:0.20027820044216074\n",
      "train loss:0.30768379708351223\n",
      "train loss:0.3966977533846094\n",
      "train loss:0.2174885078811875\n",
      "train loss:0.39214380861901565\n",
      "train loss:0.22478481571621928\n",
      "train loss:0.3972272595821955\n",
      "train loss:0.23471666252139628\n",
      "train loss:0.3148924608372967\n",
      "train loss:0.20305868634057653\n",
      "train loss:0.2857821527557719\n",
      "train loss:0.18824339727046704\n",
      "train loss:0.3708125271920866\n",
      "train loss:0.3834941006705748\n",
      "train loss:0.36285425084356604\n",
      "=== epoch:4, train acc:0.896, test acc:0.89 ===\n",
      "train loss:0.3785045492380075\n",
      "train loss:0.21662970594584785\n",
      "train loss:0.3907920435299402\n",
      "train loss:0.25713273183097196\n",
      "train loss:0.27196032521863667\n",
      "train loss:0.3016815188132684\n",
      "train loss:0.48256944241673333\n",
      "train loss:0.313907842731433\n",
      "train loss:0.23728703781465815\n",
      "train loss:0.22890908735547977\n",
      "train loss:0.5077749004823306\n",
      "train loss:0.4154247151541863\n",
      "train loss:0.20630298967642485\n",
      "train loss:0.20165851602404036\n",
      "train loss:0.2561741652634018\n",
      "train loss:0.25626526507720115\n",
      "train loss:0.34977160358481735\n",
      "train loss:0.20647242352280212\n",
      "train loss:0.2464408500967974\n",
      "train loss:0.31086408947737854\n",
      "train loss:0.24746504402904038\n",
      "train loss:0.1873613818976198\n",
      "train loss:0.23385941312909\n",
      "train loss:0.19745105197349705\n",
      "train loss:0.26837245605355625\n",
      "train loss:0.38349053665408284\n",
      "train loss:0.2764688476172583\n",
      "train loss:0.21250695251385707\n",
      "train loss:0.3132360491139495\n",
      "train loss:0.2490403714106754\n",
      "train loss:0.16954605712447074\n",
      "train loss:0.23429610404612775\n",
      "train loss:0.3527512326536897\n",
      "train loss:0.19530482578747935\n",
      "train loss:0.26230797325345206\n",
      "train loss:0.18408431016025517\n",
      "train loss:0.24403225363919132\n",
      "train loss:0.38439991101759136\n",
      "train loss:0.20528158111557118\n",
      "train loss:0.2101867814805545\n",
      "train loss:0.30206987708655014\n",
      "train loss:0.2713882596678036\n",
      "train loss:0.18996134680687693\n",
      "train loss:0.3492607200259211\n",
      "train loss:0.18892801932974163\n",
      "train loss:0.2015937986398268\n",
      "train loss:0.42982885373916324\n",
      "train loss:0.33292688952256255\n",
      "train loss:0.2830238513622572\n",
      "train loss:0.2500878253489088\n",
      "=== epoch:5, train acc:0.915, test acc:0.893 ===\n",
      "train loss:0.23378081831214015\n",
      "train loss:0.25208132653896537\n",
      "train loss:0.2381853213212556\n",
      "train loss:0.17772030697803895\n",
      "train loss:0.3180294497092935\n",
      "train loss:0.2713286477688432\n",
      "train loss:0.17628255235976126\n",
      "train loss:0.1510051128058967\n",
      "train loss:0.1390133095287525\n",
      "train loss:0.2777332761876485\n",
      "train loss:0.22356349495099642\n",
      "train loss:0.19073919621841742\n",
      "train loss:0.36873821047677824\n",
      "train loss:0.22671370877669553\n",
      "train loss:0.29797380010115326\n",
      "train loss:0.18677235392852812\n",
      "train loss:0.3824639853437922\n",
      "train loss:0.298164518534231\n",
      "train loss:0.2200243140363896\n",
      "train loss:0.16354116183529308\n",
      "train loss:0.15507985269209518\n",
      "train loss:0.21822850778583555\n",
      "train loss:0.24747216992002918\n",
      "train loss:0.19474863319902672\n",
      "train loss:0.21770314192726137\n",
      "train loss:0.2053975470931074\n",
      "train loss:0.209078550460015\n",
      "train loss:0.2297922423961823\n",
      "train loss:0.1480078762407193\n",
      "train loss:0.23102387232253668\n",
      "train loss:0.3249886222805325\n",
      "train loss:0.14674768599657015\n",
      "train loss:0.18259052282819874\n",
      "train loss:0.3755098936569642\n",
      "train loss:0.20103367971894456\n",
      "train loss:0.19774887852169054\n",
      "train loss:0.17154277045589258\n",
      "train loss:0.23638205428369072\n",
      "train loss:0.20729880924082322\n",
      "train loss:0.15750928405021403\n",
      "train loss:0.3971812642799957\n",
      "train loss:0.15107147658303502\n",
      "train loss:0.24744744243424532\n",
      "train loss:0.1425718130600776\n",
      "train loss:0.3656349976120895\n",
      "train loss:0.16090166501899053\n",
      "train loss:0.2101599718956834\n",
      "train loss:0.2878420369753012\n",
      "train loss:0.16754884524666858\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.901\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5 #20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28),  conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1}, hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データとテストデータの認識率には大きな差がないため学習が行えていることが分かる. ただし, テストデータの認識率が90.1%とあまり高いとは言えなかった. これはSImpleConvNet　の層が薄いからだと考えた."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感想\n",
    "今回から自然言語処理編を始めたので今までのところをしっかりと復習しておきたい."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "斎藤 康毅　『ゼロから作るDeep Learning』, 2019, オライリー・ジャパン, p.205-238"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "6e525390-568e-4570-8ae4-1ecf9e85c1fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
